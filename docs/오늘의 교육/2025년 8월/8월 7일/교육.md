---
layout: default
title: 
parent: 8월 7일
nav_order: 2
---

# 2025년 8월 7일 교육 내용

오늘 배운 내용을 여기에 정리합니다.

Docker Swarm (도커 스웜)

스웜이란?
**스웜 모드(Swarm mode)**는 여러 대의 도커 호스트(서버)를 하나로 묶어서 마치 하나의 거대한 도커 엔진처럼 관리할 수 있게 해주는 기능입니다. 이를 통해 여러 서버에 걸쳐 컨테이너를 배포하고 관리할 수 있습니다.

스웜의 핵심 구성 요소
노드(Node): 스웜에 참여하는 모든 서버를 '노드'라고 부릅니다. 각 노드는 도커 엔진을 실행하고 있습니다.
매니저 노드(Manager Node): 스웜 클러스터의 "두뇌" 역할을 합니다. 컨테이너의 배포, 확장, 상태 관리 등 모든 오케스트레이션 작업을 담당합니다. 매니저 노드가 다운되면 다른 매니저 노드가 그 역할을 이어받아 고가용성(High Availability)을 보장합니다.
워커 노드(Worker Node): "일꾼" 역할을 하는 노드입니다. 매니저 노드의 지시에 따라 실제로 컨테이너를 실행하고 관리합니다.

왜 스웜을 사용하는가?
스웜을 사용하면 다음과 같은 장점을 얻을 수 있습니다.
고가용성(High Availability): 컨테이너를 여러 노드에 분산시켜 배포하므로, 특정 서버가 다운되어도 서비스가 중단되지 않습니다.
로드 밸런싱(Load Balancing): 들어오는 트래픽을 여러 컨테이너에 자동으로 분배하여 안정적인 서비스를 제공합니다.
확장성(Scalability): 필요한 경우 컨테이너의 개수를 쉽게 늘리거나 줄일 수 있습니다.
간단히 말해, 컨테이너를 단일 서버에서 관리하는 것이 아니라 여러 서버에서 효율적으로 관리하고 싶을 때 스웜을 사용합니다.

--------------------------------

## 도커 스웜 클러스터 구축 및 컨테이너 서비스 관리 실습

### 1단계: 스웜 클러스터 초기 설정
호스트 이름 및 IP 설정: sudo vi /etc/hostname과 sudo vi /etc/hosts를 통해 각 서버의 호스트 이름(docker, node1, node2)과 IP 주소(192.168.0.x)를 명확하게 매핑합니다.

SSH 공개키 복사: ssh-keygen으로 공개키를 만들고, ssh-copy-id를 이용해 매니저 노드에서 워커 노드들로 복사합니다. 이를 통해 비밀번호 입력 없이 SSH 접속이 가능해져 작업 효율을 높입니다.

도커 엔진 상태 확인: sudo systemctl status docker 및 sudo systemctl status containerd를 통해 도커 서비스가 정상적으로 실행 중인지 확인합니다.

<hr>

### 2단계: 스웜 클러스터 초기화 및 노드 추가
매니저 노드 초기화: docker swarm init --advertise-addr 192.168.0.215 명령어로 docker 서버를 스웜 매니저 노드로 지정하여 클러스터를 초기화합니다.

워커 노드 합류: 초기화 후 출력되는 docker swarm join 토큰을 복사하여 node1과 node2 터미널에서 실행합니다. 이 명령어를 통해 두 서버가 스웜 클러스터의 워커 노드로 합류합니다.

노드 목록 확인: docker node ls 명령어로 스웜에 속한 모든 노드(docker, node1, node2)의 상태와 역할을 확인합니다.

<hr>

### 3단계: 스웜 서비스 생성 및 관리
오버레이 네트워크 생성: docker network create -d overlay my-overlay-network를 실행하여 여러 노드에 분산된 컨테이너 간의 통신을 가능하게 하는 가상 네트워크를 만듭니다.

서비스 생성: docker service create 명령어로 worldvit/swarmlbc:latest 이미지를 사용해 lb_test라는 이름의 서비스를 생성하고 3개의 복제본(--replicas 3)을 배포합니다.

서비스 확인: docker service ls와 docker service ps lb_test를 통해 서비스의 상태와 각 컨테이너(태스크)가 어느 노드에 배포되었는지 확인합니다.

로드 밸런싱 테스트: curl http://192.168.0.215 명령어를 반복 실행하여 스웜이 요청을 여러 컨테이너로 분산 처리하는지 확인합니다.

서비스 스케일링: docker service scale lb_test=6 명령어로 서비스의 복제본 개수를 6개로 늘려 서비스의 확장성을 테스트합니다.

서비스 삭제: docker service rm lb_test 명령어로 서비스를 클러스터에서 완전히 제거합니다.

<hr>

### 4단계: 서비스 업데이트 및 노드 관리
Nginx 서비스 배포: docker service create --replicas 5 --name web --update-delay 10s nginx:1.14 명령어로 Nginx 서비스를 5개의 복제본으로 배포합니다.

서비스 업데이트: docker service update --image nginx:1.15 web 명령어로 실행 중인 nginx의 버전을 1.14에서 1.15로 업데이트합니다. --update-delay 10s 설정에 따라 10초의 간격을 두고 순차적으로 업데이트가 진행됩니다.

노드 가용성 변경:

docker node update --availability pause node2: node2 노드가 더 이상 새로운 태스크를 받지 않도록 일시 중지(pause)합니다.

docker node update --availability active node2: node2를 다시 활성(active) 상태로 돌려놓습니다.

docker node update --availability drain node2: node2의 모든 컨테이너를 다른 노드로 이동시키고, 새로운 태스크를 받지 못하게 합니다.

스케일링과 노드 가용성 테스트: docker service scale 명령어를 통해 노드 가용성 상태에 따라 컨테이너가 어떻게 재배치되는지 확인합니다. pause 또는 drain 상태의 노드에는 컨테이너가 배포되지 않거나, 다른 노드로 이동합니다.


ubuntu@docekr : 192.168.0.215 
ubuntu@node1 : 192.168.0.218
ubuntu@node2 : 192.168.0.
* sudo vi /etc/hostname에서 유저네임 변경가능

docker, node1, node2 터미널

sudo vi /etc/hosts

```
127.0.0.1 localhost
127.0.1.1 docker
192.168.0.215 docker docker.labs.loca.
192.168.0.218 node1 docker-node1.labs.local
192.168.0.234 node2 docker-node2.labs.local


# The following lines are desirable for IPv6 capable hosts
::1     ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
```

sudo systemctl status docker
sudo systemctl status containerd

docker 터미널

ssh-keygen

ssh-copy-id ubuntu@node1
ssh-copy-id ubuntu@node2

* 둘중 아무거나 사용해도 됨
```
docker swarm init --advertise-addr $(hostname -I)
docker swarm init --advertise-addr 192.168.0.215
```

* 여기 나오는 토큰 전부 복사하기
```
docker swarm join --token
```

* 토큰 붙여넣기
```
ssh ubuntu@node1
docker swarm join --token
exit

ssh ubuntu@node2
docker swarm join --token
exit
```

* 스웜(Swarm) 클러스터에 속한 모든 노드들의 목록과 상태를 보여주는 명령어
docker node ls

* 오버레이 네트워크는 여러 도커 노드(서버)에 걸쳐 컨테이너들을 연결해주는 가상 네트워크
```
docker network create -d overlay my-overlay-network
```

docker network ls

```
docker system info | more
docker system info | grep Swarm
```

* 도커 스웜(Swarm) 클러스터에 배포된 서비스의 목록을 보여주는 명령어
```
docker service ls
```

* 도커 스웜(Swarm) 클러스터에 새로운 서비스를 생성하는 명령어
```
docker service create --name lb_test --replicas 3 -p 80:80 \
worldvit/swarmlbc:latest
```

docker service ls

curl http://192.168.0.215

* 명령어는 lb_test라는 서비스의 상세 정보를 JSON 형식으로 보여줍니다.
```
docker service inspect lb_test
```

* 명령어는 lb_test라는 서비스의 복제본(replica) 개수를 6개로 늘리거나 줄입니다.
```
docker service scale lb_test=6
```

* 명령어는 lb_test라는 서비스를 스웜 클러스터에서 완전히 삭제
```
docker service rm lb_test
```

* 명령어는 web이라는 이름의 서비스를 5개의 복제본으로 생성하고, 향후 업데이트 시 10초의 지연 시간을 설정
```
docker service create \
--replicas 5 \
--name web \
--update-delay 10s \
nginx:1.14
```

* 명령어는 web 서비스에 사용되는 이미지를 nginx:1.14에서 nginx:1.15로 변경
```
docker service update --image nginx:1.15 web
```

* 명령어는 web 서비스에 속한 모든 컨테이너(태스크)의 목록과 상태를 보여줍니다.
```
docker service ps web
```

* 설명: node2 노드를 pause 상태로 만듭니다.
  역할: pause 상태가 된 노드는 새로운 컨테이너(태스크)를 할당받지 않습니다.
```
docker node update --availability pause node2
```

docker service scale web=10
docker service ps web

* 설명: node2 노드를 다시 active 상태로 만듭니다.
  역할: pause 또는 drain 상태였던 노드를 정상적인 작업 상태로 되돌립니다.
```
docker node update --availability active node2
```

docker service scale web=5
docker service ps web

docker service scale web=10
docker service ps web

* 설명: node2 노드를 drain 상태로 만듭니다.
  역할: drain 상태가 된 노드는 기존에 실행 중이던 컨테이너들을 다른 노드로 이동시키고, 새로운 컨테이너를 받지 않습니다.
```
docker node update --availability drain node2
```

docker service ps web

docker service rm web

---------------------------------------------------------------------------------------------

## 도커 스웜 서비스를 이용한 웹 애플리케이션 배포 및 포트 노출 실습

1. 서비스 생성 (컨테이너 배포)
이 단계는 nginx 웹 서버를 3개의 복제본(--replicas 3)으로 스웜 클러스터에 배포하는 과정입니다.

docker service create: **web**이라는 이름의 스웜 서비스를 생성하는 명령어입니다. 이 서비스는 클러스터 내의 노드들에 3개의 nginx:1.14 컨테이너를 분산시켜 실행합니다.

--publish published=8080,target=80 (방법1): 자세한 포트 매핑 방식입니다. 컨테이너 내부의 포트 **80**을 클러스터의 모든 노드에 있는 포트 **8080**으로 노출시킵니다.

--p 8080:80 (방법2): --publish의 축약형입니다. 결과는 동일합니다.

프로토콜 지정: published=8080,target=80/udp와 같이 target 포트 뒤에 /udp를 붙여 프로토콜을 명시할 수 있습니다. nginx는 TCP 기반이므로 이 명령어는 서비스가 정상적으로 동작하지 않습니다.

<br>

<hr>

2. 네트워크 및 연결 상태 확인
이 단계는 서비스가 배포된 후 네트워크 연결이 올바르게 구성되었는지 검증합니다.

ss -anlt: ss 명령어는 소켓 상태를 확인하는 도구입니다. 이 명령어를 실행하면 현재 접속한 서버에서 8080 포트가 LISTEN 상태로 열려있는 것을 확인할 수 있습니다. 이는 nginx 컨테이너가 다른 노드에 있더라도, 스웜의 내장 로드 밸런서(Ingress) 덕분에 모든 노드가 8080 포트로 들어오는 요청을 받을 준비가 되었다는 것을 의미합니다.

curl http://docker:8080, curl http://node1:8080, curl http://node2:8080: 이 명령어들은 서비스에 접근 가능성을 테스트합니다. docker, node1, node2 등 어떤 노드의 IP로 접속하든, 스웜은 요청을 자동으로 감지하여 실행 중인 3개의 nginx 컨테이너 중 하나로 분산시켜줍니다. 이 테스트가 모두 성공하면 서비스가 클러스터 전체에서 정상적으로 작동함을 의미합니다.

docker service inspect web \| egrep -i published: inspect는 web 서비스의 상세 설정을 보여주고, egrep -i published는 그 출력 내용 중 **published**라는 단어가 포함된 줄만 필터링합니다. 이 명령어를 통해 --publish 옵션이 설정한 포트 매핑 정보가 올바르게 적용되었는지 다시 한 번 확인할 수 있습니다.

* 둘 중 사용하고 싶은것 사용하면 됨

### 방법1
docker service create \
--name web \
--replicas 3 \
--publish published=8080,target=80 \
nginx:1.14

### 방법2
docker service create \
--name web \
--replicas 3 \
--p 8080:80 \
nginx:1.14

* 프로토콜 지정 가능 (축약은 안됨)
```
docker service create \
--name web \
--replicas 3 \
--publish published=8080,target=80/udp \
nginx:1.14
```

ss -anlt

curl http://docker:8080
curl http://node1:8080
curl http://node2:8080

docker service inspect web | egrep -i published

---------------------------------------------------------------------------------------------
docker swarm 에서 
Monitoring : 프로메테우스 + 그라파나

cAdvisor (google)
도커로 모니터링을 웹으로 옮기고 컴포즈와 swarm을 활용해서 제작

## Docker Swarm 모니터링 시스템 구축: Prometheus, Grafana, cAdvisor 활용 가이드

### 1단계: 프로젝트 구조 생성
모니터링 스택을 구성하기 위한 파일들을 체계적으로 관리하기 위해 다음과 같은 디렉토리 구조를 생성합니다.
```
monitoring/
├── docker-compose.yml
└── prometheus/
    └── prometheus.yml
```
* monitoring/: 프로젝트의 최상위 디렉토리입니다.
* docker-compose.yml: 모니터링 스택을 정의할 Docker Compose 파일입니다.
* prometheus/: Prometheus 관련 설정 파일을 저장할 디렉토리입니다.
* prometheus.yml: Prometheus의 설정 파일입니다.

mkdir -p ./monitoring/prometheus
touch docker-compose.yml

### 2단계: Prometheus 설정 (prometheus.yml)
prometheus/prometheus.yml 파일을 열고 다음 내용을 작성합니다. 이 설정은 Docker Swarm의 서비스 디스커버리 기능을 사용하여 모니터링 대상을 동적으로 찾아냅니다.

cd prometheus
vi prometheus.yml

```
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'cadvisor'
    dns_sd_configs:
      - names:
          - 'tasks.cadvisor'
        type: 'A'
        port: 8080

  - job_name: 'node-exporter' # Node Exporter를 추가할 경우
    dns_sd_configs:
      - names:
          - 'tasks.node-exporter'
        type: 'A'
        port: 9100
```
설정 설명:
* job_name: 'cadvisor': cAdvisor로부터 메트릭을 수집하는 작업(job)을 정의합니다.
* dns_sd_configs: Docker Swarm 내부의 DNS를 사용하여 서비스를 찾도록 설정합니다. 
   tasks.cadvisor는 cadvisor라는 서비스의 모든 태스크(컨테이너)를 의미합니다.
* job_name: 'node-exporter': (선택 사항) 각 노드의 호스트 OS 메트릭(CPU, 메모리, 디스크 등)을 수집하는 Node Exporter를 위한 설정입니다.

### 3단계: Docker Compose 파일 작성 (docker-compose.yml)
프로젝트의 루트 디렉토리에 docker-compose.yml 파일을 생성하고 다음 내용을 작성합니다. 이 파일은 Prometheus, Grafana, cAdvisor를 Docker Swarm 서비스로 정의합니다.

cd ..

vi docker-compose.yml

```
services:
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    ports:
      - "9090:9090"
    networks:
      - monitoring
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure

  grafana:
    image: grafana/grafana-oss:latest
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
    networks:
      - monitoring
    depends_on:
      - prometheus
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:rw
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    networks:
      - monitoring
    deploy:
      mode: global # 모든 노드에서 실행
      restart_policy:
        condition: on-failure

volumes:
  grafana_data:

networks:
  monitoring:
    driver: overlay
```

설정 설명:
1. services:
    * prometheus: 위에서 작성한 prometheus.yml 설정을 사용하여 실행됩니다. 매니저 노드에 1개의 복제본으로 배포됩니다.
    * grafana: 3000번 포트로 접근 가능하며, 데이터는 grafana_data 볼륨에 영속적으로 저장됩니다.
    * cadvisor: global 모드로 배포되어 Swarm 클러스터의 모든 노드에서 실행됩니다. 각 노드의 컨테이너 정보를 수집하기 위해  
      호스트의 여러 디렉토리를 마운트합니다.
2. volumes:
    * grafana_data: Grafana의 대시보드 및 설정 데이터를 저장하기 위한 볼륨입니다.
3. networks:
    * monitoring: 서비스들이 서로 통신할 수 있도록 overlay 네트워크를 생성합니다.

### 4단계: 모니터링 스택 배포
터미널에서 docker-compose.yml 파일이 있는 디렉토리로 이동한 후, 다음 명령어를 실행하여 Docker Swarm에 모니터링 스택을 배포합니다.

docker stack deploy -c docker-compose.yml monitoring
    * monitoring은 스택의 이름입니다.
배포 후, 다음 명령어로 서비스가 정상적으로 실행되고 있는지 확인할 수 있습니다.
docker stack services monitoring

### 5단계: 웹 인터페이스 접속 및 설정
1. Prometheus 웹 UI 접속
    웹 브라우저에서 http://<매니저_노드_IP>:9090으로 접속하여 Prometheus UI를 확인합니다. 'Status' -> 'Targets' 메뉴로 이동하면 cadvisor 잡(job)에 등록된 노드들의 목록을 볼 수 있습니다.

2. Grafana 접속 및 설정
    웹 브라우저에서 http://<매니저_노드_IP>:3000으로 접속합니다.
    초기 아이디와 비밀번호는 admin / admin 입니다. 로그인 후 비밀번호 변경을 권장합니다.

데이터 소스 추가:
    왼쪽 메뉴에서 톱니바퀴 아이콘 (Configuration) > 'Data Sources'를 클릭합니다.
    'Add data source'를 클릭하고 'Prometheus'를 선택합니다.
    HTTP 섹션의 URL 필드에 http://prometheus:9090을 입력합니다. (Docker Swarm의 내부 DNS를 사용합니다)
    'Save & Test'를 클릭하여 연결을 확인합니다.

3. Grafana 대시보드 가져오기
    Grafana는 다른 사용자들이 만들어 놓은 유용한 대시보드를 쉽게 가져와 사용할 수 있습니다. Docker 모니터링을 위한 인기 있는 대시보드를 추가해 보겠습니다.
    왼쪽 메뉴에서 '+' 아이콘 (Create) > 'Import'를 클릭합니다.
    'Import via grafana.com' 필드에 대시보드 ID를 입력합니다. Docker Swarm 모니터링에 유용한 대시보드 ID는 다음과 같습니다.
    Docker Swarm & Container Overview: 609 또는 893
    'Load' 버튼을 클릭하고, 다음 화면에서 Prometheus 데이터 소스를 선택한 후 'Import'를 클릭합니다.

이제 Grafana 대시보드를 통해 Docker Swarm 클러스터의 컨테이너 및 노드 상태를 실시간으로 모니터링할 수 있습니다.

