---
layout: default
title: 쿠버네티스 핵심 5대 리소스 완벽 분석
parent: 8월 18일
nav_order: 2
---

# 2025년 8월 18일 교육 내용

# 쿠버네티스

## 쿠버네티스 핵심 리소스 총정리
### 쿠버네티스는 컨테이너화된 애플리케이션을 배포, 확장 및 관리하기 위한 오픈소스 플랫폼입니다. 이때 사용자가 원하는 상태를 YAML 파일 등으로 정의하면, 쿠버네티스가 그 상태를 유지하기 위해 동작합니다. 다음은 그 핵심적인 구성 요소(리소스)들입니다.

1. 레이블과 셀렉터 (Labels & Selectors)
모든 컨트롤러의 기본이 되는 개념입니다.

레이블(Label): Pod와 같은 리소스에 첨부하는 Key-Value 쌍의 꼬리표입니다. (예: app: nginx, tier: frontend) 리소스를 식별하고 그룹화하는 데 사용됩니다.

셀렉터(Selector): 특정 레이블이 붙은 리소스를 찾아내고 선택하는 규칙입니다. 컨트롤러는 셀렉터를 이용해 자신이 관리할 Pod 그룹을 정의합니다.

❗️ 중요 정보 수정: RC와 RS의 가장 큰 차이점은 셀렉터 능력입니다.

ReplicationController (RC): environment = production처럼 정확히 일치하는 조건만 사용할 수 있습니다.

ReplicaSet (RS): environment in (production, qa) 와 같이 집합 기반의 더 풍부한 조건(In, NotIn, Exists, DoesNotExist)을 사용할 수 있습니다.

2. 워크로드 리소스 (Workload Resources) - Pod 관리
애플리케이션의 종류와 목적에 따라 Pod를 관리하는 다양한 방식이 있습니다.


1) ReplicationController (RC) - 1세대 Pod 관리자
역할: 지정된 수(replicas)의 Pod가 항상 실행되도록 보장합니다. Pod가 실패하거나 삭제되면 자동으로 다시 생성하여 개수를 맞춥니다. (자가 치유)

상태: 현재는 거의 사용되지 않는 구버전(Legacy) 리소스입니다. ReplicaSet과 Deployment로 대체되었습니다.

YAML
```
# rc-nginx-example.yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx-rc
spec:
  replicas: 3
  selector:
    # 이 RC가 관리할 Pod를 선택하는 규칙입니다.
    # 'app' 키에 'nginx' 값을 가진 라벨을 찾습니다.
    app: nginx
  template:
    # 이 템플릿을 사용하여 Pod를 생성합니다.
    metadata:
      # Pod에 부여할 라벨입니다.
      # 이 라벨은 반드시 위 selector의 규칙과 일치해야 합니다.
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80 # Nginx 컨테이너가 80번 포트를 사용하도록 지정
```

2) ReplicaSet (RS) - 2세대 Pod 관리자
역할: RC와 동일하게 Pod의 개수를 보장하지만, 더 강력하고 유연한 셀렉터(matchExpressions)를 제공합니다.

상태: 보통 단독으로 사용하기보다는, Deployment의 하위 리소스로서 Deployment에 의해 자동으로 관리됩니다.

YAML
```
# rs-example.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    # 관리할 Pod를 선택하는 규칙입니다.
    # 아래 두 조건을 모두 만족해야 합니다 (AND 관계).
    matchLabels:
      tier: frontend
    matchExpressions:
      - {key: environment, operator: In, values: [production, qa]}
  template:
    # 이 템플릿을 사용하여 Pod를 생성합니다.
    metadata:
      # Pod에 부여할 라벨입니다.
      # 이 라벨은 반드시 위 selector의 규칙을 만족해야 합니다.
      labels:
        tier: frontend
        environment: production
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google_samples/gb-frontend:v3
        ports:
        - containerPort: 80
```

3) Deployment - 3세대 (현재 표준 배포 방식)
역할: ReplicaSet과 Pod의 전체 배포 수명 주기를 관리합니다. 애플리케이션의 무중단 업데이트(Rolling Update)와 롤백(Rollback) 기능을 제공하는 핵심 리소스입니다.

계층 구조: Deployment가 ReplicaSet을 관리하고, ReplicaSet이 Pod를 관리하는 구조입니다. (Deployment → ReplicaSet → Pod)

YAML
```
# deployment-example.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    # 이 Deployment가 관리할 Pod를 선택하는 규칙입니다.
    matchLabels:
      app: nginx
  template:
    # 이 템플릿을 사용하여 Pod를 생성합니다.
    # Deployment의 이미지를 변경하면, 이 템플릿이 업데이트된 새로운
    # ReplicaSet이 생성되어 롤링 업데이트가 시작됩니다.
    metadata:
      # Pod에 부여할 라벨입니다.
      # 이 라벨은 위 selector의 규칙과 일치해야 합니다.
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
```

4) DaemonSet - 모든 노드의 감시자
역할: 클러스터의 모든 (또는 특정) 노드에 Pod가 하나씩 실행되도록 보장합니다. 노드가 클러스터에 추가되면 해당 Pod를 자동으로 배포하고, 노드가 제거되면 Pod도 정리합니다.

주요 용도:
로그 수집기: 모든 노드의 로그를 수집 (예: Fluentd, Logstash)
모니터링 에이전트: 모든 노드의 성능을 감시 (예: Prometheus Node Exporter)
네트워크/스토리지 플러그인: 각 노드에 필요한 핵심 기능 제공

5) StatefulSet - 상태 저장 애플리케이션 관리자
역할: 데이터베이스(MySQL, PostgreSQL)나 메시지 큐(Kafka)처럼 상태(State)를 저장하고, 각 Pod가 고유한 식별성을 가져야 하는 애플리케이션을 위해 설계되었습니다.

주요 특징:

안정적인 고유 ID: Pod가 재시작되어도 db-0, db-1 과 같은 예측 가능한 이름을 유지합니다.

안정적인 스토리지: 각 Pod는 자신만의 영구 볼륨(Persistent Volume)에 연결되어, 재시작되어도 자신의 데이터를 그대로 다시 사용합니다.

순차적인 배포/확장: Pod를 0, 1, 2... 순서대로 생성하고, 삭제할 때는 역순(...2, 1, 0)으로 진행하여 안정성을 보장합니다.

3. 배치 리소스 (Batch Resources) - 일회성/예약 작업
1) Job - 한 번 실행하고 끝나는 작업
역할: 하나 이상의 Pod를 실행하여 주어진 작업이 성공적으로 완료될 때까지 보장합니다. 작업이 완료되면 더 이상 Pod를 생성하지 않습니다.

주요 용도: 데이터베이스 마이그레이션, 배치 처리, 데이터 변환 등 한 번만 실행하면 되는 작업.

2) CronJob - 예약된 작업
역할: 리눅스의 cron처럼 정해진 스케줄에 따라 Job을 반복적으로 실행합니다.

주요 용도: 주기적인 데이터 백업, 리포트 생성, 시스템 정리 등 예약이 필요한 모든 작업.

4. 추가하면 좋은 핵심 개념
1) 볼륨 (Volume) - 데이터 보존을 위한 외장하드
컨테이너는 기본적으로 일회용이라서 재시작되면 내부 데이터가 사라집니다. 볼륨은 Pod에 연결하는 저장 공간으로, 컨테이너가 재시작되어도 데이터를 안전하게 보존해 줍니다.

임시 볼륨 (emptyDir): Pod가 살아있는 동안만 유지되는 임시 공간.

영구 볼륨 (PersistentVolume): Pod가 사라져도 데이터가 영구적으로 보존되는 외부 스토리지. (NFS, 클라우드 디스크 등)

2) 스테이트리스 vs 스테이트풀 (Stateless vs. Stateful)
스테이트리스(Stateless): 어느 Pod로 요청이 가도 상관없는 애플리케이션 (예: 웹 서버). 아무 Pod나 지우거나 늘려도 서비스에 영향이 없습니다. Deployment로 관리하기에 이상적입니다.

스테이트풀(Stateful): 각 Pod가 자신만의 고유한 데이터와 상태를 가지는 애플리케이션 (예: 데이터베이스). Pod를 마음대로 지우거나 순서 없이 켜면 데이터 정합성이 깨질 수 있습니다. StatefulSet으로 관리해야 합니다.

3) 유용한 명령어
kubectl explain: 쿠버네티스 리소스의 구조와 각 필드에 대한 공식 문서를 터미널에서 바로 확인할 수 있는 매우 유용한 명령어입니다.

Bash
```
# Pod 리소스의 최상위 필드 설명 보기
kubectl explain pod

# Pod의 spec 필드 하위 구조 보기
kubectl explain pod.spec
```



--------------------------------------------------------------------------

# 🚀 ReplicationController를 이용한 Pod 그룹 관리 실습
## 🎯 목표: ReplicationController(RC)를 사용하여 Pod 그룹의 개수를 일정하게 유지(자가 치유)하고, Pod 그룹을 확장하거나 업데이트하는 방법을 익힙니다.

## 💡 핵심 개념: ReplicationController(RC)는 지정된 수의 Pod 복제본이 항상 실행되도록 보장하는 **'Pod 관리자'**입니다. RC는 두 가지 핵심 요소로 동작합니다.

replicas: 유지해야 할 Pod의 정확한 개수

selector: 관리할 Pod를 식별하기 위한 라벨(Label) 조건

참고: 현재 RC는 Deployment라는 더 새롭고 강력한 리소스로 대체되었습니다. 하지만 RC의 동작 원리를 이해하면 Deployment를 더 깊이 있게 파악할 수 있으므로 매우 중요한 개념입니다.

단계별 실습
1) 준비: YAML 파일 생성
먼저 실습에 사용할 RC 정의 파일을 만듭니다.

Bash
```
# 실습을 위한 디렉토리 생성 및 이동
mkdir rc-lab && cd rc-lab
```
rc-nginx.yaml 파일을 아래 내용으로 생성하세요. app=webui 라벨을 가진 Nginx Pod를 항상 3개 유지하도록 설정합니다.

vi rc-nginx.yaml
YAML
```
apiVersion: v1
kind: ReplicationController
metadata:
  name: rc-nginx
spec:
  replicas: 3
  selector:
    app: webui # 이 라벨을 가진 Pod를 관리 대상으로 삼음
  template:
    metadata:
      name: nginx-pod # Pod 템플릿의 이름 (실제 Pod 이름에 사용되지는 않음)
      labels:
        app: webui # Pod에 이 라벨을 부여함
    spec:
      containers:
      - name: nginx
        image: nginx:1.14
        ports:
        - containerPort: 80
```

2) RC 생성 및 상태 확인
작성한 YAML 파일을 이용해 RC와 Pod들을 생성합니다.

Bash
```
kubectl apply -f rc-nginx.yaml
```
RC와 그에 의해 생성된 Pod들이 정상적으로 실행되는지 확인합니다.

Bash
```
# RC의 상태 확인
kubectl get rc

# Pod 목록과 각 Pod의 라벨 확인
kubectl get pods --show-labels
```
    출력 예상:
    ```
    NAME       DESIRED   CURRENT   READY   AGE
    rc-nginx   3         3         3       10s
    NAME             READY   STATUS    RESTARTS   AGE   LABELS
    rc-nginx-abcde   1/1     Running   0          10s   app=webui
    rc-nginx-fghij   1/1     Running   0          10s   app=webui
    rc-nginx-klmno   1/1     Running   0          10s   app=webui
    ```

3) 자가 치유(Self-Healing) 기능 확인
RC의 가장 중요한 기능인 자가 치유를 테스트합니다. Pod 중 하나를 강제로 삭제해 보세요.

Bash
```
# Pod 이름은 위에서 확인한 실제 이름으로 바꿔주세요.
kubectl delete pod rc-nginx-abcde
```
잠시 후 Pod 목록을 다시 확인하면, 삭제된 Pod를 대신할 새로운 Pod가 즉시 생성되어 다시 3개를 유지하는 것을 볼 수 있습니다.

Bash
```
kubectl get pods
```

4) 레이블 통제(Label Control) 확인
RC가 자신의 라벨(app=webui)을 얼마나 엄격하게 관리하는지 확인해 봅시다. 동일한 라벨을 가진 redis Pod를 수동으로 생성해 봅니다.

vim redis-pod.yaml
YAML
```
# redis-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: redis
  labels:
    app: webui # <-- RC가 관리하는 라벨과 동일
spec:
  containers:
  - name: redis
    image: redis
```
이 파일을 적용하면 어떤 일이 일어날까요?

Bash
```
kubectl apply -f redis-pod.yaml
kubectl get pods
```
결과: redis Pod가 생성되자마자 Terminating(종료 중) 상태가 되는 것을 볼 수 있습니다. RC가 app=webui 라벨을 가진 4번째 Pod를 발견하고, replicas: 3 규칙을 지키기 위해 즉시 제거해 버리기 때문입니다.

5) 스케일링 - Pod 개수 변경
가장 간단한 방법은 scale 명령어를 사용하는 것입니다. Pod 개수를 5개로 늘려봅시다.

Bash
```
kubectl scale rc rc-nginx --replicas=5
```
YAML 파일을 직접 수정하여 개수를 변경할 수도 있습니다.

Bash
```
# RC 설정을 편집기로 엽니다.
kubectl edit rc rc-nginx
# replicas 값을 5에서 2로 변경하고 저장합니다.
```

Pod 목록을 확인하여 개수가 변경되었는지 확인합니다.
Bash
```
$ kubectl get pods
```
6) 수동 롤링 업데이트 - 이미지 변경
RC의 Pod 템플릿에서 이미지 버전을 변경하여 Pod를 업데이트해 봅시다.

Bash
```
# 'nginx:1.14' 이미지를 'nginx:1.15'로 변경
kubectl set image rc/rc-nginx nginx=nginx:1.15
```
중요한 점은, RC는 이미지를 변경해도 기존 Pod를 자동으로 재시작하지 않는다는 것입니다. 변경 사항을 적용하려면 수동으로 Pod를 삭제해야 합니다.

Bash
```
# 구버전 Pod 중 하나를 삭제
kubectl delete pod <구버전-pod-이름>
```
삭제 후 새로 생성되는 Pod의 상세 정보를 확인하면, 이미지 버전이 1.15로 변경된 것을 볼 수 있습니다.

Bash
```
# 새로 생성된 Pod의 이름으로 확인
$ kubectl describe pod <새버전-pod-이름> | grep "Image:"
```

7) 실습 마무리: RC 삭제
마지막으로 실습에 사용한 리소스를 정리합니다.

Bash
```
# RC와 RC가 관리하던 모든 Pod를 함께 삭제
kubectl delete rc rc-nginx
```

만약 RC만 삭제하고 Pod들은 남겨두고 싶다면 --cascade=orphan 옵션을 사용합니다.
Bash
```
# RC만 삭제하고 Pod는 그대로 둡니다. (Pod들은 '고아' 상태가 됨)
kubectl delete rc rc-nginx --cascade=orphan
```

--------------------------------------------------------------------------

ReplicaSet

# 🚀 ReplicaSet 실습: 더 강력한 Selector 활용하기 
## 🎯 목표: ReplicationController(RC)의 후속 버전인 ReplicaSet(RS)을 생성하고, 더 다양하고 강력한 라벨 조건(matchExpressions)을 사용하는 방법을 익힙니다.

## 💡 핵심 개념: ReplicaSet은 RC와 거의 동일하게 Pod의 개수를 일정하게 유지하는 역할을 하지만, 더 발전된 Selector를 제공하는 것이 가장 큰 차이점입니다.

ReplicationController vs ReplicaSet 비교
| 기능 | ReplicationController | ReplicaSet |
| :--- | :---: | :---: |
| **Pod 개수 유지** | ✅ | ✅ |
| **matchLabels 지원** | ✅ | ✅ |
| **matchExpressions 지원** | ❌ | ✅ |
| **자동 롤링 업데이트** | ❌ | ❌ (Deployment 필요) |

이처럼 ReplicaSet의 진정한 가치는 matchExpressions를 통한 정교한 Pod 선택 능력에 있습니다.

matchExpressions 연산자 개념 이해
Selector의 각 연산자가 어떤 Pod를 선택하는지 간단한 예시로 알아봅시다.

In 연산자: version이 1.14 또는 1.15인 Pod를 모두 선택합니다.
    Selector: {version In (1.14, 1.15)}
    
    Pod A {version: 1.14} -> (매칭됨)
    Pod B {version: 1.15} -> (매칭됨)
    Pod C {version: 1.16} -> (매칭 안됨)

NotIn 연산자: env가 dev가 아닌 Pod를 모두 선택합니다.
    Selector: {env NotIn (dev)}

    Pod A {env: prod} -> (매칭됨)
    Pod B {env: qa} -> (매칭됨)
    Pod C {env: dev} -> (매칭 안됨)

Exists 연산자: cacheable이라는 라벨 키가 있기만 하면 값을 따지지 않고 선택합니다.
Selector: {Exists (cacheable)}

    Pod A {cacheable: true} -> (매칭됨)
    Pod B {cacheable: false} -> (매칭됨)
    Pod C {} -> (매칭 안됨)

단계별 실습
1) ReplicaSet YAML 파일 작성
matchExpressions를 사용하여 app=webui 라벨을 가지면서 동시에 version 라벨이 1.14 또는 1.15인 Pod를 3개 관리하는 RS를 정의합니다.

vi rs-nginx.yaml
YAML
```
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: rs-nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webui
    matchExpressions:
      - {key: version, operator: In, values: ["1.14", "1.15"]}
  template:
    metadata:
      labels:
        app: webui
        version: "1.14" # Pod 생성 시 사용할 라벨
    spec:
      containers:
      - name: nginx
        image: nginx:1.14
        ports:
        - containerPort: 80
```

2) RS 생성 및 상태 확인
작성한 YAML 파일을 클러스터에 적용하고, 생성된 RS와 Pod들을 확인합니다.

Bash
```
kubectl apply -f rs-nginx.yaml
kubectl get rs
kubectl get pods --show-labels
```
    출력 예상: 3개의 Pod가 app=webui, version=1.14 라벨을 가지고 생성됩니다.

3) Selector 동작 심층 분석
이제 다른 라벨을 가진 Pod를 만들어 RS가 어떻게 반응하는지 확인해 봅시다.

    사례 1: 일부만 일치하는 Pod 생성 (관리 대상 아님)
    app=webui 라벨은 만족하지만, matchExpressions 조건(version 라벨)이 없는 Pod를 만들어 봅시다.

Bash
```
kubectl run ignored-pod --image=redis --labels="app=webui"
```
    결과: 이 Pod는 Selector의 두 가지 조건을 모두 만족하지 않으므로, RS는 이 Pod를 무시하고 아무런 조치도 취하지 않습니다.

사례 2: 모든 조건이 일치하는 Pod 생성 (관리 대상 포함)
이번에는 RS의 Selector 조건에 완벽하게 부합하는 Pod를 만들어 봅시다.

Bash
```
kubectl run extra-pod --image=redis --labels="app=webui,version=1.15"
```
    결과: extra-pod가 생성되자마자 즉시 Terminating 됩니다. RS가 자신의 관리 조건에 맞는 4번째 Pod를 발견하고, replicas: 3 규칙을 지키기 위해 즉시 제거하기 때문입니다.

4) 실습 마무리
실습에 사용했던 모든 리소스를 정리합니다.

Bash
```
kubectl delete rs rs-nginx
kubectl delete pod ignored-pod
```

## 💡 실무 Best Practice 코멘트
실제 운영 환경에서는 이 실습처럼 ReplicaSet을 직접 생성하는 경우는 거의 없습니다.

ReplicaSet은 주로 Deployment의 하위 리소스로서, Deployment에 의해 자동으로 생성되고 관리됩니다. 우리는 Deployment에 "어떤 앱을 몇 개 실행하고, 어떻게 업데이트할지"만 알려주면, Deployment가 내부적으로 ReplicaSet을 활용하여 Pod의 개수를 보장하고 무중단 업데이트를 처리해 줍니다.

따라서 ReplicaSet의 강력한 Selector 개념을 이해하는 것은 매우 중요하지만, 직접 다루기보다는 Deployment를 통해 간접적으로 활용하는 것이 일반적인 Best Practice입니다.

--------------------------------------------------------------------------

# 🚀 Deployment 실습: 무중단 애플리케이션 배포 마스터하기
## 🎯 목표: ReplicaSet을 자동으로 제어하며 애플리케이션의 **무중단 업데이트(Rolling Update)**와 **롤백(Rollback)**을 관리해주는 Deployment의 강력한 기능을 실습합니다.

## 💡 핵심 개념: Deployment는 애플리케이션 배포의 최종 진화형입니다. 개발자는 Deployment에 **"어떤 앱을 몇 개 실행하고, 어떻게 업데이트할지"**라는 **원하는 상태(Desired State)**만 선언하면, Deployment가 알아서 ReplicaSet과 Pod를 만들고, 업데이트가 필요할 때 자동으로 롤링 업데이트를 수행합니다.

컨트롤러 비교: RC, RS, 그리고 Deployment
| 기능 | ReplicationController | ReplicaSet | **Deployment** |
| :--- | :---: | :---: | :---: |
| **Pod 개수 유지** | ✅ | ✅ | ✅ |
| **`matchExpressions` 지원** | ❌ | ✅ | ✅ (통해 ReplicaSet) |
| **자동 롤링 업데이트** | ❌ | ❌ | ✅ |
| **배포 기록 및 롤백** | ❌ | ❌ | ✅ |

단계별 실습
📜 실습 흐름 요약
생성: Deployment를 생성하면 ReplicaSet과 Pod가 자동으로 만들어지는 계층 구조를 확인합니다.

업데이트: 이미지 버전을 변경하여 서비스 중단 없이 Pod가 교체되는 롤링 업데이트를 체험합니다.

기록 확인: 배포가 어떻게 변경되었는지 이력을 조회합니다.

롤백: 문제가 발생한 상황을 가정하고, 이전 버전으로 안전하게 되돌아갑니다.

스케일링: Pod 개수를 자유롭게 조절합니다.

1) Deployment YAML 파일 작성
먼저 실습에 사용할 기본 Deployment 정의 파일을 생성합니다.

vi dep-nginx.yaml
YAML
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dep-nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  strategy:
    type: RollingUpdate # 업데이트 전략 지정
    rollingUpdate:
      maxSurge: 25%       # 업데이트 중 정해진 개수보다 최대 25% 초과 가능
      maxUnavailable: 25% # 업데이트 중 최대 25%의 Pod가 사용 불가 상태일 수 있음
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14
        ports:
        - containerPort: 80
```

2) Deployment 생성 및 구조 확인
Deployment를 생성하고, 그 결과로 ReplicaSet과 Pod가 함께 생성되는 것을 확인합니다.

Bash
```
kubectl apply -f dep-nginx.yaml
```
Bash
```
# Deployment, ReplicaSet, Pod가 계층 구조로 생성된 것을 확인
kubectl get deploy,rs,po
```
    출력 예상: dep-nginx Deployment가 dep-nginx-xxxx ReplicaSet을 만들고, 이 ReplicaSet이 3개의 Pod를 관리하는 구조를 볼 수 있습니다.

배포가 성공적으로 완료되었는지 상태를 확인합니다.

Bash
```
kubectl rollout status deployment/dep-nginx
```

3) 무중단 롤링 업데이트 (Rolling Update) 체험
Deployment의 가장 중요한 기능입니다. 터미널 2개를 열고 한쪽에서는 실시간으로 Pod의 변화를 관찰합니다.

터미널 1 (모니터링용):
Bash
```
kubectl get pods --watch
```

터미널 2 (실행용): set image 명령어로 Nginx 이미지 버전을 1.16으로 업데이트합니다.

Bash
```
kubectl set image deployment/dep-nginx nginx=nginx:1.16
```

    ✨ 핵심 관찰 포인트: 터미널 1에서 신버전(1.16) Pod가 하나 생성되면 구버전(1.14) Pod가 하나씩 Terminating 되는 점진적인 업데이트 과정이 자동으로 진행되는 것을 직접 눈으로 확인하세요. 이 과정 동안 서비스는 중단되지 않습니다.

4) 배포 기록(History) 확인 및 롤백(Rollback)
업데이트가 완료된 후, 배포 기록을 확인하고 문제가 생겼을 경우를 가정하여 이전 버전으로 되돌리는 롤백을 수행합니다.

Bash
```
# 배포 기록 확인 (REV 1은 초기 생성, REV 2는 1.16으로 업데이트한 기록)
kubectl rollout history deployment/dep-nginx
```
    💡 확인 팁: kubectl describe deployment dep-nginx 명령어를 실행하면, Events 섹션에서 새로운 ReplicaSet을 만들고 구버전 ReplicaSet의 개수를 줄여나가는(Scaling down) 과정을 상세히 볼 수 있습니다.

이제 명령 한 줄로 이전 버전으로 롤백합니다.

Bash
```
# 바로 이전 버전(REV 1)으로 롤백
kubectl rollout undo deployment/dep-nginx
```
--watch를 실행 중인 터미널에서, 이번에는 1.14 버전 Pod가 새로 생기고 1.16 버전 Pod가 사라지는 롤백 과정이 자동으로 진행되는 것을 확인할 수 있습니다.

5) 스케일링 (Scaling)
Deployment의 Pod 개수를 조절하는 것은 매우 간단합니다.
Bash
```
# Pod 개수를 5개로 확장 (Scale-out)
$ kubectl scale deployment dep-nginx --replicas=5
```

Bash
```
# Pod 개수를 다시 3개로 축소 (Scale-in)
$ kubectl scale deployment dep-nginx --replicas=3
```

6) 실습 마무리
Deployment를 삭제하면 하위의 ReplicaSet과 Pod가 모두 한 번에 정리됩니다.
Bash
```
kubectl delete deployment dep-nginx
```

--------------------------------------------------------------------------

# 🚀 DaemonSet 실습: 모든 노드에 Pod 배포하고 업데이트하기
## 🎯 목표: 클러스터의 모든 워커 노드에 Pod를 하나씩 배포하고, 자가 치유 기능과 무중단 업데이트(Rolling Update) 전략을 직접 확인하며 DaemonSet의 역할을 완벽히 이해합니다.

## 💡 핵심 개념: DaemonSet은 **"모든 노드에 하나씩"**이라는 규칙을 보장하는 컨트롤러입니다. 클러스터에 새로운 노드가 추가되면 해당 Pod를 자동으로 배포하고, 노드가 사라지면 Pod도 함께 정리합니다.

Deployment/ReplicaSet vs DaemonSet 비교
| 기능 | Deployment / ReplicaSet | **DaemonSet** |
| :--- | :--- | :--- |
| **Pod 제어 방식** | `replicas` 수만큼 Pod 개수 유지 | 모든 노드에 Pod 1개씩 실행 보장 |
| **`replicas` 필드** | ✅ **필수** | ❌ **사용 안 함** |
| **주요 용도** | 상태 없는(Stateless) 웹 애플리케이션 | 로그 수집기, 모니터링 에이전트 |
| **업데이트 방식** | **RollingUpdate** (정교한 제어 가능) | **RollingUpdate** 또는 **OnDelete** |

단계별 실습
1) 준비: 실습 환경 정리
이전 실습에서 생성했을 수 있는 리소스를 정리하여 깨끗한 환경에서 시작합니다.

Bash
```
# 기존에 실행 중인 Deployment가 있다면 모두 삭제
kubectl delete deployment --all
```

2) DaemonSet YAML 파일 작성
모든 워커 노드에 Nginx Pod를 하나씩 배포하는 DaemonSet을 정의합니다. 업데이트 전략(updateStrategy)도 명시적으로 포함합니다.

vi dae-nginx.yaml
YAML
```
# dae-nginx.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: dae-nginx
spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1 # 업데이트 시 한 번에 최대 1개의 Pod만 비활성화
  selector:
    matchLabels:
      app: nginx-agent
  template:
    metadata:
      labels:
        app: nginx-agent
    spec:
      containers:
      - name: nginx
        image: nginx:1.14
        ports:
        - containerPort: 80
```

3) DaemonSet 생성 및 상태 확인
작성한 YAML 파일을 클러스터에 적용하고, 각 워커 노드에 Pod가 하나씩 생성되었는지 확인합니다.
Bash
```
$ kubectl apply -f dae-nginx.yaml
```

Bash
```
# -o wide 옵션으로 어느 노드에 Pod가 배포되었는지 확인하는 것이 핵심입니다.
kubectl get daemonset
kubectl get pods -o wide
```

    출력 예상: 클러스터의 워커 노드 개수만큼 dae-nginx Pod가 생성되고, NODE 컬럼에 각기 다른 노드 이름이 표시됩니다.

4) 자가 치유(Self-Healing) 기능 확인
DaemonSet으로 관리되는 Pod 중 하나를 강제로 삭제하여, 컨트롤러가 어떻게 반응하는지 관찰합니다.

Bash
```
# Pod 이름은 위에서 확인한 실제 이름으로 바꿔주세요.
kubectl delete pod <pod-name>
```
잠시 후 Pod 목록을 다시 확인하면, Pod가 삭제되었던 바로 그 노드에 새로운 Pod가 즉시 다시 생성되는 것을 볼 수 있습니다.

5) 업데이트 전략(RollingUpdate) 확인
이제 DaemonSet의 이미지 버전을 변경하여 무중단 업데이트가 어떻게 진행되는지 직접 확인해 봅시다.

터미널 1 (모니터링용): Pod의 변화를 실시간으로 관찰합니다.
Bash
```
kubectl get pods -o wide --watch
```

터미널 2 (실행용): set image 명령어로 이미지 버전을 1.16으로 업데이트합니다.
Bash
```
kubectl set image daemonset/dae-nginx nginx=nginx:1.16
```
    ✨ 핵심 관찰 포인트: 터미널 1에서 노드마다 순차적으로 기존 Pod가 Terminating되고 새로운 버전의 Pod가 생성되는 것을 볼 수 있습니다. maxUnavailable: 1 설정 때문에 한 번에 하나씩 안전하게 업데이트가 진행됩니다.

6. 실습 마무리
실습에 사용했던 DaemonSet을 삭제합니다.

Bash
```
kubectl delete -f dae-nginx.yaml
```

## 📜 DaemonSet 핵심 요약
역할: 클러스터의 모든 노드에 Pod를 1개씩 실행하는 것을 보장합니다.
자동화: 노드가 추가/삭제될 때 Pod를 자동으로 배포하거나 정리합니다.
replicas 없음: 노드의 수가 곧 복제본의 수이므로 replicas 필드를 사용하지 않습니다.
주요 용도: 에이전트, 로그 수집, 모니터링 등 노드 레벨에서 항상 실행되어야 하는 프로그램에 사용됩니다.

--------------------------------------------------------------------------

# 🚀 DaemonSet 무중단 배포 및 롤백 마스터하기
## 🎯 목표: 실행 중인 DaemonSet의 애플리케이션 버전을 서비스 중단 없이 안전하게 업데이트하고, 문제가 발생했을 때 특정 버전으로 정확하게 되돌리는(롤백) 방법을 완벽히 마스터합니다.

# 💡 핵심 개념:롤링 업데이트 (Rolling Update): DaemonSet의 Pod 템플릿이 변경되면, 쿠버네티스는 노드마다 순차적으로 Pod를 교체하여 서비스 중단 없이 점진적으로 업데이트를 수행합니다.

배포 기록 (Revision History): DaemonSet은 업데이트가 발생할 때마다 변경 내역을 리비전(Revision)으로 저장합니다.

롤백 (Rollback): 저장된 리비전 기록을 이용해, 문제가 발생했을 때 이전의 안정적인 버전으로 신속하게 되돌릴 수 있습니다.

단계별 실습
1) 사전 준비: 실습용 DaemonSet 배포
먼저 업데이트 및 롤백을 수행할 DaemonSet을 배포합니다. 롤백을 위해 몇 개의 버전 기록을 남길지 지정하는 revisionHistoryLimit 필드를 추가합니다.

vi dae-nginx.yaml
YAML
```
# dae-nginx.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: dae-nginx
spec:
  revisionHistoryLimit: 3 # 롤백을 위해 3개의 이전 버전 기록을 보관
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  selector:
    matchLabels:
      app: nginx-agent
  template:
    metadata:
      labels:
        app: nginx-agent
    spec:
      containers:
      - name: nginx
        image: nginx:1.14
        ports:
        - containerPort: 80
```

Bash
```
# DaemonSet 생성
$ kubectl apply -f dae-nginx.yaml

# 모든 Pod가 nginx:1.14 버전으로 정상 실행되었는지 확인
$ kubectl get pods -o wide
```

2) 롤링 업데이트 수행 (v1.14 → v1.15)
이제 Nginx 이미지 버전을 1.15로 무중단 업데이트를 진행합니다.

    (터미널 1) 업데이트 과정 실시간 모니터링:
    업데이트 과정을 생생하게 보려면, 아래 명령어를 다른 터미널에서 실행하는 것을 강력히 추천합니다.
    Bash
    ```
    kubectl get pods -o wide --watch
    ```

    (터미널 2) 업데이트 명령어 실행:
    set image 명령어로 업데이트를 트리거합니다.
    Bash
    ```
    kubectl set image daemonset/dae-nginx nginx=nginx:1.15
    ```

    ## ✨ 핵심 관찰 포인트: 터미널 1에서 노드마다 순차적으로 기존 Pod가 Terminating되고 새로운 Pod가 ContainerCreating을 거쳐 Running으로 바뀌는 과정을 직접 눈으로 확인하세요.

    (터미널 2) 업데이트 완료 대기:
    rollout status 명령어를 사용하면, 업데이트가 완료될 때까지 자동으로 기다려주어 매우 편리합니다.
    Bash
    ```
    kubectl rollout status daemonset/dae-nginx
    ```

    출력 예상: daemon set "dae-nginx" successfully rolled out

3) 업데이트 결과 확인
모든 Pod가 새로운 버전(1.15)으로 실행 중인지 확인하고, 배포 기록이 추가되었는지 조회합니다.

Bash
```
# Pod 목록을 통해 모든 Pod의 상태와 버전을 확인 (AGE가 짧은 것이 새 Pod)
$ kubectl get pods -o wide

# 배포 기록(history) 확인
$ kubectl rollout history daemonset/dae-nginx
```
    출력 예상: REVISION 1번(1.14)과 2번(1.15)이 생성된 것을 볼 수 있습니다.

4) 롤백(Rollback) 수행
업데이트된 버전에 문제가 있다고 가정하고, 이전의 안정적인 1.14 버전으로 되돌아갑니다.

방법 1: 바로 이전 버전으로 롤백
가장 간단한 롤백 방법입니다.
Bash
```
# rollout undo 명령어로 이전 버전으로 롤백
$ kubectl rollout undo daemonset dae-nginx

# 롤백이 완료될 때까지 대기
$ kubectl rollout status daemonset/dae-nginx
```

방법 2 (심화): 특정 리비전으로 롤백
여러 번의 업데이트 후, 바로 직전이 아닌 특정 버전(예: 최초 버전인 REVISION 1)으로 한 번에 돌아가고 싶을 때 사용합니다.

    💡 핵심 포인트: --to-revision 옵션을 사용하면 "무조건 직전 버전만 복구 가능하다"는 오해를 막고, 원하는 시점으로 정확하게 돌아갈 수 있습니다.

먼저, 다시 1.15 버전으로 업데이트하여 리비전을 2개 이상으로 만듭니다.

Bash
```
$ kubectl set image daemonset/dae-nginx nginx=nginx:1.15
$ kubectl rollout status daemonset/dae-nginx
$ kubectl rollout history daemonset/dae-nginx
```

이제, 현재 버전에서 최초 버전인 **REVISION 1**으로 직접 롤백합니다.
Bash
```
kubectl rollout undo daemonset/dae-nginx --to-revision=1

# 롤백이 완료될 때까지 대기
$ kubectl rollout status daemonset/dae-nginx
```

5) 롤백 결과 확인
모든 Pod가 다시 1.14 버전으로 돌아왔는지 최종 확인합니다.
Bash
```
$ kubectl describe daemonset dae-nginx | grep Image:
```
    출력 예상: Image: nginx:1.14

6) 실습 마무리
실습에 사용했던 DaemonSet을 삭제하여 환경을 정리합니다.
Bash
```
kubectl delete daemonset dae-nginx
```

--------------------------------------------------------------------------

 NodePort

# 🚀 StatefulSet 실습: 상태 저장 애플리케이션 완벽 마스터하기
## 🎯 목표: 데이터베이스와 같이 안정적인 이름과 데이터 저장이 필요한 '상태 저장(Stateful)' 애플리케이션을 배포하는 StatefulSet의 고유한 특징을 완벽히 이해하고 직접 관리해 봅니다.

## 💡 핵심 개념: StatefulSet은 각 Pod가 고유한 정체성을 가지는 애플리케이션을 위해 설계되었습니다.

고유하고 안정적인 네트워크 ID:
Pod가 st-nginx-0, st-nginx-1처럼 예측 가능한 순번으로 생성되며, 재시작되어도 이름이 절대 바뀌지 않습니다.

이를 위해 반드시 Headless Service가 필요합니다. Headless Service는 각 Pod에게 st-nginx-0.st-nginx-svc.default.svc.cluster.local과 같은 고유하고 안정적인 DNS 주소를 부여합니다.

고유하고 안정적인 스토리지:
각 Pod는 volumeClaimTemplates를 통해 자신만의 고유한 저장 공간(PVC)을 자동으로 할당받습니다. Pod가 재시작되어도 이 PVC에 다시 연결되어 데이터를 보존합니다.

Access Modes:
ReadWriteOnce (RWO): 한 번에 하나의 노드(Pod)만 볼륨에 읽고 쓸 수 있습니다. (가장 일반적인 모드)

ReadWriteMany (RWX): 여러 노드(Pod)에서 동시에 볼륨에 읽고 쓸 수 있습니다. (NFS 같은 공유 스토리지 필요)

순차적인 배포와 관리 (podManagementPolicy):
OrderedReady (기본값): Pod를 순서대로(0, 1, 2...) 생성/업데이트하고, 삭제할 때는 역순(...2, 1, 0)으로 진행하여 안정성을 보장합니다.

Parallel: 모든 Pod를 동시에 생성/삭제합니다. 순서 보장이 필요 없을 때 사용합니다.

단계별 실습
1) 사전 준비: Headless Service 생성
StatefulSet의 안정적인 네트워크 ID를 제공하기 위해 반드시 clusterIP: None으로 설정된 Headless Service를 먼저 생성해야 합니다.

vi st-svc.yaml
YAML
```
apiVersion: v1
kind: Service
metadata:
  name: st-nginx-svc
spec:
  clusterIP: None # Headless 서비스로 만들기 위한 설정
  selector:
    app: nginx
  ports:
  - port: 80
```

2) StatefulSet YAML 파일 작성
각 Pod가 고유한 저장 공간을 자동으로 요청하도록 volumeClaimTemplates가 포함된 StatefulSet을 정의합니다.

vi st-nginx.yaml
YAML
```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: st-nginx
spec:
  replicas: 3
  serviceName: "st-nginx-svc" # 1단계에서 만든 Headless Service 이름
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14
        ports:
        - containerPort: 80
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi
```

3) StatefulSet 생성 및 순차적 배포 확인
준비된 YAML 파일들을 적용하고, Pod가 순서대로 생성되는 과정을 관찰합니다.
Bash
```
kubectl apply -f st-svc.yaml
kubectl apply -f st-nginx.yaml
```

    Pod 생성 과정 실시간 모니터링:
    Bash
    ```
    kubectl get pods -o wide --watch
    ```
    ✨ 핵심 관찰 포인트: st-nginx-0이 Running 상태가 된 후에야 st-nginx-1이 생성되는 순차적인 배포 과정을 직접 확인하세요.

    💡 심화 학습 (DNS 확인): st-nginx-0 Pod에 접속하여 nslookup으로 각 Pod의 고유한 DNS 주소가 잘 생성되었는지 확인해 봅시다.
    Bash
    ```
    kubectl exec -it st-nginx-0 -- /bin/bash
    # --- Pod 내부에서 실행 ---
    root@st-nginx-0:/# nslookup st-nginx-1.st-nginx-svc
    ```

4) 안정적인 ID 및 데이터 영속성 확인
Pod를 삭제하고 재생성하여 이름과 데이터가 유지되는지 확인합니다.

Pod 내부에 데이터 작성: st-nginx-1 Pod에 접속하여 고유한 파일을 생성합니다.
Bash
```
kubectl exec -it st-nginx-1 -- /bin/bash -c 'echo "This is pod 1" > /usr/share/nginx/html/my-id.txt'
```

Pod 삭제:
Bash
```
$ kubectl delete pod st-nginx-1
```

재생성 및 데이터 확인:
--watch 터미널에서 st-nginx-1 Pod가 동일한 이름으로 재생성되는 것을 확인한 후, 이전에 생성한 파일이 그대로 남아있는지 확인합니다.
Bash
```
# Pod가 다시 Running 상태가 된 후 실행
kubectl exec -it st-nginx-1 -- cat /usr/share/nginx/html/my-id.txt
```
    출력 예상: This is pod 1 (데이터가 성공적으로 유지됨)

5) 순차적 스케일링 및 롤링 업데이트
스케일 아웃/인: 복제본 수를 조절하면 Pod가 순서대로(Scale-out) 또는 역순으로(Scale-in) 생성/삭제되는 것을 확인하세요.

Bash
```
kubectl scale statefulset st-nginx --replicas=4 # st-nginx-3 생성
kubectl scale statefulset st-nginx --replicas=2 # st-nginx-3, st-nginx-2 순서로 삭제
```

롤링 업데이트: 이미지 버전을 변경하면 **역순(높은 번호부터)**으로 업데이트가 진행됩니다. 이는 데이터 정합성을 보장하기 위함입니다.
Bash
```
kubectl set image statefulset/st-nginx nginx=nginx:1.15
kubectl rollout status statefulset/st-nginx
```

6) 실습 마무리
StatefulSet, 서비스, 그리고 남겨진 PVC까지 모두 삭제합니다.

Bash
```
kubectl delete statefulset st-nginx
kubectl delete service st-nginx-svc
```
    ⚠️ 중요: StatefulSet을 삭제해도 데이터 보호를 위해 PVC는 자동으로 삭제되지 않습니다. 디스크 공간을 완전히 정리하려면 아래 명령어로 PVC를 직접 삭제해야 합니다.
    Bash
    ```
    kubectl delete pvc -l app=nginx
    ```

--------------------------------------------------------------------------

# 🚀 Job 컨트롤러 실습: 일회성 작업 완벽 마스터하기
## 🎯 목표: 데이터 처리나 백업처럼 한 번 실행되고 성공적으로 완료되어야 하는 일회성 작업을 관리하는 Job 컨트롤러의 사용법과 주요 옵션을 익힙니다.

## 💡 핵심 개념: Job의 역할: Job은 내부에 정의된 작업(컨테이너)이 성공적으로 완료(Exit Code 0)될 때까지 Pod를 실행하는 것을 보장합니다. 작업이 실패하면 재시도하고, 성공하면 더 이상 Pod를 실행하지 않습니다.

restartPolicy의 중요성: Job에서 restartPolicy는 실패 시 Pod를 새로 만들지(Never), 컨테이너만 재시작할지(OnFailure) 결정하는 핵심 설정입니다.

restartPolicy 동작 방식 비교
| `restartPolicy` | 동작 방식 | Pod 개수 변화 | `RESTARTS` 횟수 변화 |
| :--- | :--- | :---: | :---: |
| **`Never`** | 컨테이너 실패 시, **새로운 Pod**를 만들어 재시도 | **증가함** (재시도 횟수만큼) | **0** (새 Pod이므로) |
| **`OnFailure`** | 컨테이너 실패 시, **기존 Pod 내에서 컨테이너만** 재시작 | **변화 없음** (Pod는 1개 유지) | **증가함** |

실습 1: 기본 Job 실행 및 restartPolicy 동작 이해
목표: 정상 Job과 실패 Job을 각각 실행하여 restartPolicy: Never와 OnFailure의 동작 차이를 명확히 이해합니다.

1-A: 정상 Job 실행 (restartPolicy: Never)
정상 Job YAML 파일 작성: 5초간 실행되다 정상 종료되는 간단한 Job을 정의합니다.

vi job-success.yaml
YAML
```
apiVersion: batch/v1
kind: Job
metadata:
  name: success-job
spec:
  template:
    spec:
      containers:
      - name: simple-task
        image: busybox
        command: ["/bin/sh", "-c", "echo 'Job Starting!'; sleep 5; echo 'Job Finished!'"]
      restartPolicy: Never
```
Job 생성 및 모니터링: Pod가 Running을 거쳐 Completed 상태로 바뀌는 것을 확인하세요.
Bash
```
kubectl apply -f job-success.yaml
kubectl get pods --watch
```

1-B: 실패 Job과 restartPolicy: Never (backoffLimit 시각화)
실패 Job YAML 파일 작성: backoffLimit: 4는 최대 4번 재시도를 의미합니다.

vi job-fail-never.yaml
YAML
```
apiVersion: batch/v1
kind: Job
metadata:
  name: fail-job-never
spec:
  backoffLimit: 4 # 최대 4번 재시도
  template:
    spec:
      containers:
      - name: failing-task
        image: busybox
        command: ["/bin/sh", "-c", "echo 'This will fail'; exit 1"]
      restartPolicy: Never
```

Job 생성 및 모니터링:
--watch 터미널에서 새로운 Pod가 계속 생성되다가 Error 또는 CrashLoopBackOff 상태가 되는 것이 반복되는 것을 관찰하세요.
Bash
```
kubectl apply -f job-fail-never.yaml
```
        ✨ backoffLimit 동작 확인: Pod가 1개(최초 실행) + 4개(재시도) = 총 5개까지 생성된 후, 더 이상 새로운 Pod가 만들어지지 않는 것을 볼 수 있습니다.

1-C: 실패 Job과 restartPolicy: OnFailure (backoffLimit 시각화)
실패 Job YAML 파일 작성: 이번에는 restartPolicy를 OnFailure로 설정합니다.

vi job-fail-onfailure.yaml
YAML
```
apiVersion: batch/v1
kind: Job
metadata:
  name: fail-job-onfailure
spec:
  backoffLimit: 4
  template:
    spec:
      containers:
      - name: failing-task
        image: busybox
        command: ["/bin/sh", "-c", "echo 'This will fail'; exit 1"]
      restartPolicy: OnFailure
```

Job 생성 및 모니터링:
--watch 터미널에서 Pod는 하나만 생성되고, 그 Pod의 RESTARTS 횟수만 계속 증가하는 것을 관찰하세요.

Bash
```
kubectl apply -f job-fail-onfailure.yaml
```

        ✨ backoffLimit 동작 확인: RESTARTS 횟수가 4번까지 올라간 후, Job 컨트롤러가 재시도를 포기하고 Pod를 최종 Error 상태로 두는 것을 볼 수 있습니다.

실습 2: Job의 병렬 실행 및 완료 조건 제어
목표: completions와 parallelism 옵션을 사용하여 여러 작업을 병렬로 실행하고, activeDeadlineSeconds로 최대 실행 시간을 제어합니다.

병렬 Job YAML 파일 작성: 총 8번의 성공적인 완료가 필요하며, 동시에 최대 2개의 Pod를 병렬로 실행하는 Job을 정의합니다.

vi job-parallel.yaml
YAML
```
apiVersion: batch/v1
kind: Job
metadata:
  name: parallel-job
spec:
  completions: 8
  parallelism: 2
  template:
    spec:
      containers:
      - name: pi-calculator
        image: perl
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: OnFailure
```

Job 생성 및 관찰:
--watch 터미널에서 Pod가 2개씩 생성되고, 하나가 Completed되면 다음 Pod가 생성되어 항상 2개의 병렬 실행을 유지하려 하는 것을 관찰하세요.
Bash
```
kubectl apply -f job-parallel.yaml
```

📜 Job 상태 최종 확인 및 정리
실습이 끝난 후, Pod가 아닌 Job 리소스 자체의 상태를 확인하여 최종 결과를 파악하는 것이 중요합니다.

Job 목록 및 최종 상태 확인:
get jobs 명령어로 각 Job의 성공 여부(COMPLETIONS)를 한눈에 볼 수 있습니다.
Bash
```
kubectl get jobs
```
        출력 예상:

        NAME                  COMPLETIONS   DURATION   AGE
        fail-job-never        0/1           2m10s      5m
        fail-job-onfailure    0/1           2m30s      4m
        parallel-job          8/8           1m5s       3m
        success-job           1/1           12s        6m

실패한 Job의 상세 원인 분석:
describe job 명령어는 Job이 실패한 이유(예: Back-off limit exceeded)를 Events 섹션에서 상세히 보여줍니다.
Bash
```
kubectl describe job fail-job-onfailure
```

실습 마무리:
모든 Job 리소스를 삭제하여 환경을 정리합니다.
Bash
```
kubectl delete jobs --all
```

--------------------------------------------------------------------------

# 🚀 CronJob 실습: 예약된 작업 자동화 마스터하기
## 🎯 목표: 리눅스의 cron처럼 정해진 스케줄에 따라 주기적으로 작업을 실행하는 CronJob의 사용법을 익히고, 주요 옵션을 통해 동작을 제어하는 방법을 실습합니다.

## 💡 핵심 개념: 계층 구조: CronJob은 Job을 관리하는 컨트롤러입니다. 사용자가 원하는 시간(schedule)이 되면, CronJob은 Job 리소스를 생성하고, 생성된 Job이 Pod를 실행하여 실제 작업을 수행합니다. (CronJob → Job → Pod)

스케줄 형식: CronJob의 schedule 필드는 5개의 필드로 구성된 표준 크론 형식을 따릅니다.

```
# ┌───────────── 분 (0 - 59)
# │ ┌───────────── 시 (0 - 23)
# │ │ ┌───────────── 일 (1 - 31)
# │ │ │ ┌───────────── 월 (1 - 12)
# │ │ │ │ ┌───────────── 요일 (0 - 6) (일요일=0)
# │ │ │ │ │
# * * * * *
```

| 표현식 | 의미 |
| :--- | :--- |
| `* * * * *` | 1분마다 실행 |
| `*/5 * * * *` | 5분마다 실행 |
| `0 3 * * *` | 매일 새벽 3시 0분에 실행 |
| `0 3 * * 1-5` | 평일(월요일~금요일) 새벽 3시 0분에 실행 |

단계별 실습
1) Job과 CronJob YAML 비교
두 리소스의 구조를 나란히 비교하면 CronJob이 Job을 어떻게 "포장"하는지 쉽게 이해할 수 있습니다.

| Job (단일 실행) | CronJob (반복 실행) |
| :--- | :--- |
| <pre lang="yaml"><code>apiVersion: batch/v1<br>kind: Job<br>metadata:<br>  name: simple-job<br>spec:<br>  template:<br>    spec:<br>      containers:<br>      - name: task<br>        image: busybox<br>        command: ["/bin/sh", "-c", "echo Hello"]<br>      restartPolicy: OnFailure</code></pre> | <pre lang="yaml"><code>apiVersion: batch/v1<br>kind: CronJob<br>metadata:<br>  name: hello-cronjob<br>spec:<br>  schedule: "*/1 * * * *"<br>  jobTemplate:<br>    spec:<br>      template:<br>        spec:<br>          containers:<br>          - name: task<br>            image: busybox<br>            command: ["/bin/sh", "-c", "echo Hello"]<br>          restartPolicy: OnFailure</code></pre> |

        ✨ 핵심 포인트: CronJob의 .spec.jobTemplate 필드 내용은 Job의 .spec 필드 내용과 정확히 동일합니다. 즉, CronJob은 Job의 명세서(spec)를 그대로 가져와 schedule만 추가한 형태입니다.

2) CronJob 생성 및 실행 관찰
1분마다 "Hello World"를 출력하는 간단한 CronJob을 생성하고, 계층적으로 리소스가 생성되는 과정을 관찰합니다.

vi cronjob-hello.yaml
YAML
```
apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello-cronjob
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello-task
            image: busybox
            command: ["/bin/sh", "-c", "date; echo 'Hello from the CronJob'"]
          restartPolicy: OnFailure
```

CronJob 생성 및 실시간 모니터링:
watch 명령어로 세 가지 리소스(CronJob, Job, Pod)를 동시에 관찰하여 계층 구조를 한눈에 파악합니다.

Bash
```
kubectl apply -f cronjob-hello.yaml

# 다른 터미널에서 실행
watch kubectl get cronjob,job,pod
```

        ✨ 핵심 관찰 포인트: 매 분이 시작될 때마다,

        hello-cronjob이 활성화되고,
        새로운 hello-cronjob-xxxxxxxx Job이 생성되며,
        이어서 해당 Job이 실행할 Pod가 생성되었다가 Completed되는 전체 흐름을 확인하세요.

3) 심화 실습: 옵션 제어하기
concurrencyPolicy: Forbid (동시 실행 방지):
이전 작업이 끝나지 않았으면, 새로운 Job을 실행하지 않도록 설정합니다. 65초가 걸리는 작업을 1분 주기로 실행하여 테스트합니다.

vi cronjob-policy.yaml
YAML
```
apiVersion: batch/v1
kind: CronJob
metadata:
  name: policy-cronjob
spec:
  schedule: "*/1 * * * *"
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: long-task
            image: busybox
            command: ["/bin/sh", "-c", "sleep 65"]
          restartPolicy: OnFailure
```
        결과 확인: watch 터미널에서 첫 Job이 65초 동안 실행되는 동안에는 다음 스케줄이 되어도 새로운 Job이 생성되지 않는 것을 확인하세요.

History Limits (실행 기록 개수 제어):
성공/실패한 Job의 기록을 몇 개까지 남길지 설정합니다.

vi cronjob-history.yaml
YAML
```
apiVersion: batch/v1
kind: CronJob
metadata:
  name: history-cronjob
spec:
  schedule: "*/1 * * * *"
  successfulJobsHistoryLimit: 5 # 성공 기록 5개 보관
  failedJobsHistoryLimit: 2   # 실패 기록 2개 보관
  jobTemplate:
    # ... job spec ...
```

        결과 확인: 이 CronJob을 실행하고 몇 분 뒤 kubectl get jobs를 실행하면, Completed 상태의 Job이 5개를 초과하지 않는 것을 볼 수 있습니다.

4) 실습 마무리
실습에 사용했던 모든 CronJob 리소스를 삭제하여 환경을 정리합니다. CronJob을 삭제하면 관련된 Job들도 함께 정리됩니다.

Bash
```
kubectl delete cronjob --all
```