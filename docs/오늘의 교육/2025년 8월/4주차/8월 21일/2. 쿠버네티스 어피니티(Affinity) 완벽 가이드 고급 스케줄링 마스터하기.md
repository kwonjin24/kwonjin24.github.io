---
layout: default
title: 쿠버네티스 어피니티(Affinity) 완벽 가이드: 고급 스케줄링 마스터하기
parent: 8월 21일
nav_order: 3
---

# 2025년 8월 20일 교육 내용

# 쿠버네티스

## 📌 쿠버네티스 어피니티(Affinity) 완벽 정리
1. 어피니티(Affinity)란 무엇인가?
어피니티는 **"선호도"**라는 뜻으로, Pod를 특정 조건을 만족하는 **노드(Node)**에 배치하거나, 특정 다른 Pod와 함께 혹은 떨어뜨려 배치하도록 쿠버네티스 스케줄러에게 알려주는 고급 스케줄링 규칙입니다.

👉 쉬운 비유: 팀 회의실 배정하기
여러 팀(Pod)이 회의실(Node)을 예약해야 합니다.

Node Affinity: "우리 팀은 꼭 창가 쪽 큰 회의실에서 회의해야 해!" (노드의 특정 조건 선호)

Pod Affinity: "우리 팀은 마케팅팀과 가까운 회의실로 잡아줘." (다른 Pod와의 위치 관계 선호)

Pod Anti-Affinity: "우리 팀은 경쟁사인 라이벌팀과는 무조건 다른 층으로 잡아줘." (다른 Pod와 떨어지려는 선호)

2. 노드 어피니티 (Node Affinity)
새로운 Pod를 특정 조건을 가진 노드에 배치하도록 설정합니다. nodeSelector보다 훨씬 더 유연하고 다양한 규칙을 만들 수 있습니다.

nodeSelector와의 차이점
nodeSelector는 size=large처럼 정확히 일치하는 라벨만 지정할 수 있습니다.

Node Affinity는 size In (large, medium) (크기가 large 또는 medium인), core Gt 2 (코어 수가 2보다 큰) 와 같이 **다양한 연산자(In, NotIn, Exists, Gt, Lt 등)**를 사용할 수 있어 훨씬 표현력이 풍부합니다.

노드 어피니티의 두 가지 규칙 (강제 vs 선호)
requiredDuringSchedulingIgnoredDuringExecution (강제 규칙 / Hard)

"반드시 이 규칙을 만족하는 노드에만 Pod를 배치해라."

만약 조건을 만족하는 노드가 하나도 없다면, Pod는 배치되지 않고 영원히 Pending 상태로 남습니다.

nodeSelector와 동일한 강제성을 가집니다.

preferredDuringSchedulingIgnoredDuringExecution (선호 규칙 / Soft)

"가급적 이 규칙을 만족하는 노드에 배치하되, 그런 노드가 없으면 다른 곳에라도 배치해라."

weight (가중치, 1~100)를 설정하여 여러 선호 규칙 중 우선순위를 정할 수 있습니다.

조건을 만족하는 노드가 없더라도 Pod는 스케줄링됩니다.

3. 파드 어피니티 & 안티-어피니티 (Pod Affinity & Anti-Affinity)
이미 실행 중인 다른 Pod를 기준으로, 새로운 Pod의 위치를 결정하는 규칙입니다.

1) 파드 어피니티 (Pod Affinity) - "같이 있으면 좋아"
역할: 특정 Pod들을 서로 같은 위치에 배치하려고 할 때 사용합니다.

사용 시나리오: 웹 서버와 캐시 서버처럼 서로 자주 통신하여 네트워크 지연 시간을 최소화해야 할 때, 이 둘을 같은 노드에 배치하면 성능상 이점이 있습니다.

2) 파드 안티-어피니티 (Pod Anti-Affinity) - "같이 있으면 안 돼"
역할: 특정 Pod들을 서로 다른 위치에 분산하여 배치하려고 할 때 사용합니다.

사용 시나리오 (매우 중요): **고가용성(High Availability, HA)**을 구성할 때 주로 사용합니다. 예를 들어, 동일한 애플리케이션의 복제본(Replica) 3개를 서로 다른 노드에 분산시키면, 노드 하나에 장애가 발생해도 다른 노드에 있는 Pod들이 서비스를 계속 이어갈 수 있습니다.

파드 어피니티/안티-어피니티의 두 가지 규칙 (강제 vs 선호)
노드 어피니티와 마찬가지로, **강제 규칙(required...)**과 선호 규칙(preferred...) 두 가지를 모두 제공하여 유연하게 스케줄링 정책을 설정할 수 있습니다.

💡 실무 Best Practice:
실제 환경에서는 **강제 규칙(Hard Affinity)**보다 **선호 규칙(Soft Affinity)**을 사용하는 것이 더 권장되는 경우가 많습니다. 강제 규칙을 너무 엄격하게 사용하면, 조건을 만족하는 노드/Pod가 없어 새로운 Pod가 아예 배포되지 못하는 상황이 발생할 수 있기 때문입니다. 선호 규칙은 가용성을 최대한 확보하면서 원하는 방향으로 스케줄링을 유도하는 더 안정적인 방법입니다.

--------------------------------------------------------------------------

🚀 노드 어피니티(Node Affinity) 실습: 특정 노드에 Pod 강제 배포하기 (Required)
🎯 목표: required 노드 어피니티 규칙을 사용하여, 특정 라벨(Label)을 가진 노드에만 Pod가 반드시 배포되도록 설정하는 방법을 실습합니다.

💡 핵심 개념:

requiredDuringSchedulingIgnoredDuringExecution:

requiredDuringScheduling: Pod를 스케줄링(배치)할 때, 이 규칙은 반드시 지켜져야 합니다. 만약 조건을 만족하는 노드가 단 하나도 없다면, Pod는 배포되지 않고 영원히 Pending 상태로 남습니다.

IgnoredDuringExecution: Pod가 이미 실행 중인 상태에서 노드의 라벨이 변경되어 규칙을 만족하지 않게 되더라도, 실행 중인 Pod는 쫓겨나지 않고 계속 유지됩니다.

matchExpressions: nodeSelector보다 더 다양한 표현이 가능한 규칙입니다. key, operator, values를 조합하여 유연한 조건을 만들 수 있습니다.

단계별 실습
1단계: 준비 - 노드 라벨 확인 및 추가
먼저, Pod를 배포할 특정 노드를 지정하기 위해 라벨을 직접 추가합니다.

현재 노드 라벨 확인:
클러스터의 모든 노드와 각 노드가 가진 기본 라벨들을 확인합니다.

Bash

$ kubectl get nodes --show-labels
특정 노드에 라벨 추가:
node1.labs.local 노드에만 disktype=ssd 라는 특별한 라벨을 추가합니다.

Bash

$ kubectl label nodes node1.labs.local disktype=ssd
라벨 추가 결과 확인:
disktype 라벨이 node1에만 정상적으로 추가되었는지 확인합니다.

Bash

$ kubectl get nodes -L disktype
출력 예상: DISKTYPE 컬럼에 node1.labs.local만 ssd라고 표시됩니다.

NAME                STATUS   ROLES           AGE   VERSION   DISKTYPE
master.labs.local   Ready    control-plane   22d   v1.25.1   <none>
node1.labs.local    Ready    <none>          22d   v1.25.1   ssd
node2.labs.local    Ready    <none>          22d   v1.25.1   <none>
node3.labs.local    Ready    <none>          22d   v1.25.1   <none>
2단계: Required Node Affinity를 사용하는 Deployment YAML 작성
이제, disktype=ssd 라벨이 있는 노드에만 배포되도록 강제하는 Deployment를 정의합니다. 실습의 편의를 위해 3개의 복제본(replicas)을 생성하도록 설정합니다.

required-affinity-deploy.yaml 파일을 생성하세요.

YAML

# required-affinity-deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: required-affinity-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-required
  template:
    metadata:
      labels:
        app: nginx-required
    spec:
      affinity:
        nodeAffinity:
          # "반드시" 이 규칙을 만족하는 노드에만 Pod를 배포합니다.
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              # 'disktype' 키의 값이 'ssd' 목록 안에(In) 있는 노드를 찾습니다.
              - key: disktype
                operator: In
                values:
                - ssd
      containers:
      - name: nginx
        image: nginx
3단계: Deployment 생성 및 배포 결과 확인
작성한 YAML 파일을 클러스터에 적용하고, Pod들이 어느 노드에 배포되었는지 확인합니다.

Bash

$ kubectl apply -f required-affinity-deploy.yaml
잠시 후, Pod 목록을 확인합니다.

Bash

$ kubectl get pods -o wide
✨ 핵심 확인 포인트:
replicas가 3개임에도 불구하고, 생성된 모든 Pod가 disktype=ssd 라벨을 가진 node1.labs.local 노드에만 집중적으로 배포된 것을 확인할 수 있습니다. 다른 노드들은 규칙을 만족하지 못했기 때문에 스케줄링에서 제외되었습니다.

4단계: 실습 리소스 정리
실습이 끝나면 생성했던 Deployment와 노드에 추가했던 라벨을 모두 삭제하여 환경을 원래대로 되돌립니다.

Bash

# Deployment 삭제
$ kubectl delete -f required-affinity-deploy.yaml

# 노드 라벨 삭제 (키 이름 뒤에 '-'를 붙임)
$ kubectl label nodes node1.labs.local disktype-

--------------------------------------------------------------------------

🚀 노드 어피니티(Node Affinity) 실습: 선호하는 노드에 Pod 우선 배포하기 (Preferred)
🎯 목표: preferred 노드 어피니티 규칙과 weight(가중치)를 사용하여, 특정 조건을 만족하는 노드에 Pod를 우선적으로 배포하는 방법을 실습합니다.

💡 핵심 개념:

preferredDuringSchedulingIgnoredDuringExecution:

preferredDuringScheduling: 스케줄러가 이 규칙을 최대한 지키려고 노력합니다. 각 노드는 자신이 만족하는 모든 선호 규칙의 weight 값을 합산하여 점수를 받으며, 가장 높은 점수를 받은 노드에 Pod가 우선적으로 배포됩니다.

IgnoredDuringExecution: Pod가 이미 실행 중인 상태에서 노드의 라벨이 변경되어도, 실행 중인 Pod는 영향을 받지 않습니다.

핵심 차이점: **'강제' 규칙(required)**은 조건을 만족하는 노드가 없으면 Pod를 아예 배포하지 않지만, **'선호' 규칙(preferred)**은 조건을 만족하는 노드가 없더라도 사용 가능한 다른 노드에 Pod를 배포하여 가용성을 높입니다.

단계별 실습
1단계: 준비 - 노드 라벨 추가
Pod가 선호도를 판단할 수 있도록 각기 다른 라벨을 다른 노드에 추가합니다.

node2에 라벨 추가:

Bash

$ kubectl label nodes node2.labs.local team=backend
node3에 라벨 추가:

Bash

$ kubectl label nodes node3.labs.local team=database
라벨 추가 결과 확인:
team 라벨이 각 노드에 정상적으로 추가되었는지 확인합니다.

Bash

$ kubectl get nodes -L team
출력 예상: TEAM 컬럼에 node2는 backend, node3는 database라고 표시됩니다.

2단계: Preferred Node Affinity를 사용하는 Pod YAML 작성
이제, 두 개의 선호 규칙을 가지되, 서로 다른 가중치(weight)를 부여하여 어떤 노드를 더 선호할지 정의합니다.

preferred-affinity-pod.yaml 파일을 생성하세요.

YAML

# preferred-affinity-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  affinity:
    nodeAffinity:
      # "가급적" 이 규칙들을 만족하는 노드에 Pod를 배포합니다.
      preferredDuringSchedulingIgnoredDuringExecution:
      # 규칙 1: team=backend 라벨을 가진 노드는 30점의 가중치를 가짐
      - weight: 30
        preference:
          matchExpressions:
          - key: team
            operator: In
            values:
            - backend
      # 규칙 2: team=database 라벨을 가진 노드는 80점의 가중치를 가짐
      - weight: 80
        preference:
          matchExpressions:
          - key: team
            operator: In
            values:
            - database
  containers:
  - name: nginx
    image: nginx
3단계: Pod 생성 및 배포 결과 확인
작성한 YAML 파일을 클러스터에 적용하고, Pod가 어느 노드에 배포되었는지 확인합니다.

Bash

$ kubectl apply -f preferred-affinity-pod.yaml
잠시 후, Pod 목록을 확인합니다.

Bash

$ kubectl get pod nginx -o wide
✨ 핵심 확인 포인트:
Pod가 team=database 라벨을 가진 node3.labs.local에 배포된 것을 확인할 수 있습니다. 이는 스케줄러가 각 노드의 점수를 계산할 때, node3이 만족하는 규칙의 가중치(weight: 80)가 node2가 만족하는 규칙의 가중치(weight: 30)보다 높았기 때문입니다.

4단계: 실습 리소스 정리
실습이 끝나면 생성했던 Pod와 노드에 추가했던 라벨을 모두 삭제하여 환경을 원래대로 되돌립니다.

Bash

# Pod 삭제
$ kubectl delete -f preferred-affinity-pod.yaml

# 노드 라벨 삭제
$ kubectl label nodes node2.labs.local team-
$ kubectl label nodes node3.labs.local team-

--------------------------------------------------------------------------

🚀 노드 어피니티(Node Affinity) 실습: 선호 조건 불일치 시 동작 확인
🎯 목표: preferred 노드 어피니티 규칙을 가진 Pod를 배포할 때, 선호하는 조건을 만족하는 노드가 하나도 없는 경우에도 Pod가 정상적으로 배포되는 것을 확인하고, '선호' 규칙의 유연성을 이해합니다.

💡 핵심 개념:

required (강제) vs preferred (선호)의 결정적 차이:

required: 조건을 만족하는 노드가 없으면 Pod는 배포되지 않고 영원히 Pending 상태로 남습니다.

preferred: 조건을 만족하는 노드가 없으면, 스케줄러는 어피니티 규칙을 무시하고 클러스터 내에 자원이 있는 다른 노드에 Pod를 배포합니다. 이는 Pod의 가용성을 보장하는 데 매우 유리합니다.

단계별 실습
1단계: 준비 - 노드 라벨 상태 확인
이전 실습에 이어, node2에는 team=backend 라벨이, node3에는 team=database 라벨이 붙어있는 상태에서 시작합니다. 다른 노드에는 특별한 라벨이 없습니다.

Bash

# 현재 노드들의 'team' 라벨 상태를 확인
$ kubectl get nodes -L team
2단계: 선호 조건을 만족하지 못하는 Pod YAML 작성
이제, **존재하지 않는 라벨(priority=high)**에 매우 높은 가중치(90점)를 주고, 존재하는 라벨(team=backend)에는 낮은 가중치(20점)를 주는 Pod를 정의합니다.

preferred-fallback-pod.yaml 파일을 생성하세요.

YAML

# preferred-fallback-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-fallback
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnedDuringExecution:
      # 규칙 1: 'priority=high' 라벨이 있는 노드를 90점짜리 최고 선호도로 지정
      # (현재 클러스터에는 이 노드가 없음)
      - weight: 90
        preference:
          matchExpressions:
          - key: priority
            operator: In
            values:
            - high
      # 규칙 2: 'team=backend' 라벨이 있는 노드는 20점의 선호도로 지정
      - weight: 20
        preference:
          matchExpressions:
          - key: team
            operator: In
            values:
            - backend
  containers:
  - name: nginx
    image: nginx
3단계: Pod 생성 및 배포 결과 확인
작성한 YAML 파일을 클러스터에 적용하고, Pod가 어느 노드에 배포되었는지 확인합니다.

Bash

$ kubectl apply -f preferred-fallback-pod.yaml
잠시 후, Pod 목록을 확인합니다.

Bash

$ kubectl get pod nginx-fallback -o wide
✨ 핵심 확인 포인트:
스케줄러는 가장 높은 점수(90점)를 가진 priority=high 노드를 찾으려 했지만, 그런 노드가 없었습니다. 따라서 차선책으로 두 번째로 높은 점수(20점)를 가진 node2.labs.local에 Pod를 배포한 것을 확인할 수 있습니다.

만약 node2마저 사용할 수 없는 상황이었다면, 스케줄러는 모든 선호 규칙을 무시하고 자원이 있는 다른 노드(예: node1)에 Pod를 배포했을 것입니다. 이것이 바로 preferred 규칙의 유연성입니다.

4단계: 실습 리소스 정리
실습이 끝나면 생성했던 Pod와 노드에 추가했던 라벨을 모두 삭제합니다.

Bash

# Pod 삭제
$ kubectl delete -f preferred-fallback-pod.yaml

# 노드 라벨 삭제 (이전 실습에서 추가한 라벨 모두 정리)
$ kubectl label nodes node2.labs.local team-
$ kubectl label nodes node3.labs.local team-

--------------------------------------------------------------------------

🚀 노드 어피니티(Node Affinity) 실습: 강제 규칙과 선호 규칙 조합하기
🎯 목표: required 노드 어피니티로 배포될 노드 후보군을 먼저 거르고, 그중에서 preferred 노드 어피니티를 사용해 가장 선호하는 노드에 Pod를 우선적으로 배포하는 방법을 실습합니다.

💡 핵심 개념: 2단계 스케줄링 프로세스
required와 preferred 규칙이 함께 있으면, 스케줄러는 2단계에 걸쳐 동작합니다.

필터링 (Filtering): 먼저 required... 규칙을 확인합니다. 이 규칙을 만족하는 노드들만 "후보 노드"로 남고, 나머지 노드들은 모두 탈락합니다.

스코어링 (Scoring): 그 다음, "후보 노드"들만을 대상으로 preferred... 규칙을 평가합니다. 각 후보 노드는 자신이 만족하는 선호 규칙의 weight(가중치)를 합산하여 점수를 받으며, 가장 높은 점수를 받은 후보 노드에 Pod가 최종적으로 배포됩니다.

단계별 실습
1단계: 준비 - 시나리오에 맞는 노드 라벨 설정
실습을 위해 각기 다른 지역(region)과 가용 영역(zone)을 나타내는 라벨을 노드에 설정합니다.

node1: 필수 조건과 선호 조건을 모두 만족 (가장 유력한 후보)

node2: 필수 조건은 만족하지만 선호 조건은 만족하지 못함 (차선책 후보)

node3: 필수 조건조차 만족하지 못함 (탈락 대상)

Bash

# node1에 라벨 추가
$ kubectl label nodes node1.labs.local region=us-east-1 zone=us-east-1a

# node2에 라벨 추가
$ kubectl label nodes node2.labs.local region=us-east-1 zone=us-east-1b

# node3에 라벨 추가
$ kubectl label nodes node3.labs.local region=us-west-2 zone=us-west-2a
라벨이 정상적으로 추가되었는지 확인합니다.

Bash

$ kubectl get nodes -L region,zone
2단계: 강제 및 선호 규칙을 모두 사용하는 Deployment YAML 작성
이제, **"반드시 region=us-east-1에 배포하되, 가급적이면 zone=us-east-1a에 배포하라"**는 규칙을 가진 Deployment를 정의합니다.

combined-affinity-deploy.yaml 파일을 생성하세요.

YAML

# combined-affinity-deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: combined-affinity-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-combined
  template:
    metadata:
      labels:
        app: nginx-combined
    spec:
      affinity:
        nodeAffinity:
          # 1단계 (필터링): "반드시" 이 규칙을 만족하는 노드만 후보로 선정
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: region
                operator: In
                values:
                - us-east-1
          # 2단계 (스코어링): 후보 노드 중에서 이 규칙을 만족하는 곳을 선호
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80 # 높은 가중치
            preference:
              matchExpressions:
              - key: zone
                operator: In
                values:
                - us-east-1a
      containers:
      - name: nginx
        image: nginx
3단계: Deployment 생성 및 배포 결과 확인
작성한 YAML 파일을 클러스터에 적용하고, Pod들이 어느 노드에 배포되었는지 확인합니다.

Bash

$ kubectl apply -f combined-affinity-deploy.yaml
잠시 후, Pod 목록을 확인합니다.

Bash

$ kubectl get pods -o wide
✨ 핵심 확인 포인트:

node3은 필수 조건(region=us-east-1)을 만족하지 못해 아예 후보에서 제외되었습니다.

남은 후보(node1, node2) 중에서, 선호 규칙(zone=us-east-1a)까지 만족하여 가장 높은 점수를 받은 node1.labs.local에 모든 Pod가 우선적으로 배포된 것을 확인할 수 있습니다.

만약 node1에 자원이 부족했다면, 차선책으로 node2.labs.local에 배포되었을 것입니다.

4단계: 실습 리소스 정리
실습이 끝나면 생성했던 Deployment와 노드에 추가했던 라벨을 모두 삭제합니다.

Bash

# Deployment 삭제
$ kubectl delete -f combined-affinity-deploy.yaml

# 노드 라벨 삭제
$ kubectl label nodes node1.labs.local region- zone-
$ kubectl label nodes node2.labs.local region- zone-
$ kubectl label nodes node3.labs.local region- zone-

--------------------------------------------------------------------------

🚀 파드 어피니티 & 안티-어피니티 실습
🎯 목표: 이미 실행 중인 다른 Pod의 라벨을 기준으로, 새로운 Pod를 특정 Pod와 **가깝게 배치(Affinity)**하거나 **멀리 분산하여 배치(Anti-Affinity)**하는 방법을 실습합니다.

💡 파드 어피니티/안티-어피니티 핵심 개념
노드 어피니티가 노드의 라벨을 기준으로 Pod의 위치를 결정했다면, 파드 어피니티는 다른 Pod의 라벨을 기준으로 위치를 결정합니다.

1) 파드 어피니티 (Pod Affinity) - "같이 있으면 좋아"
역할: 특정 Pod들을 서로 같은 위치에 배치하려고 할 때 사용합니다.

사용 시나리오: 웹 서버와 캐시 서버처럼 서로 자주 통신하여 네트워크 지연 시간을 최소화해야 할 때, 이 둘을 같은 노드(또는 같은 데이터센터 존)에 배치하면 성능상 이점이 있습니다.

2) 파드 안티-어피니티 (Pod Anti-Affinity) - "같이 있으면 안 돼"
역할: 특정 Pod들을 서로 다른 위치에 분산하여 배치하려고 할 때 사용합니다.

사용 시나리오 (매우 중요): **고가용성(High Availability, HA)**을 구성할 때 주로 사용합니다. 예를 들어, 동일한 애플리케이션의 복제본(Replica) 3개를 서로 다른 노드에 분산시키면, 노드 하나에 장애가 발생해도 다른 노드에 있는 Pod들이 서비스를 계속 이어갈 수 있습니다.

3) topologyKey - "위치"의 기준
topologyKey는 '가깝다' 또는 '멀다'의 기준이 되는 **위치(Topology)**를 정의합니다.

kubernetes.io/hostname: 같은 **노드(서버)**에 있는가?

topology.kubernetes.io/zone: 같은 **가용 영역(Availability Zone)**에 있는가?

topology.kubernetes.io/region: 같은 **리전(Region)**에 있는가?

⚠️ 주의: 파드 안티-어피니티를 안정적으로 사용하려면, 클러스터의 모든 노드에 topologyKey로 사용할 라벨(예: zone 라벨)이 일관되게 적용되어 있어야 합니다.

단계별 실습
1단계: 기준이 될 '앵커(Anchor)' Pod 배포
먼저, 어피니티 규칙의 기준점이 될 security=S1 라벨을 가진 Nginx Pod를 하나 배포합니다.

anchor-pod.yaml 파일을 생성하세요.

YAML

# anchor-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: anchor-pod
  labels:
    security: S1
spec:
  containers:
  - name: nginx
    image: nginx
앵커 Pod 배포 및 위치 확인:

Bash

$ kubectl apply -f anchor-pod.yaml

# 잠시 후, anchor-pod가 어느 노드에 배포되었는지 확인합니다.
$ kubectl get pods -o wide
2단계: 파드 어피니티를 사용하는 Pod YAML 작성
이제, **"반드시 security=S1 라벨을 가진 Pod와 같은 노드에 배포하라"**는 강제(required) 파드 어피니티 규칙을 가진 새로운 Pod를 정의합니다.

affinity-pod.yaml 파일을 생성하세요.

YAML

# affinity-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: affinity-pod
spec:
  affinity:
    podAffinity:
      # "반드시" 이 규칙을 만족하는 위치에 Pod를 배포합니다.
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          # 'security' 키의 값이 'S1'인 Pod를 찾습니다.
          - key: security
            operator: In
            values:
            - S1
        # 위치의 기준은 '노드'입니다 (같은 서버).
        topologyKey: kubernetes.io/hostname
  containers:
  - name: my-app
    image: redis
3단계: Pod 생성 및 배포 결과 확인
작성한 YAML 파일을 클러스터에 적용하고, affinity-pod가 anchor-pod와 같은 노드에 배포되었는지 확인합니다.

Bash

$ kubectl apply -f affinity-pod.yaml
잠시 후, Pod 목록을 확인합니다.

Bash

$ kubectl get pods -o wide
✨ 핵심 확인 포인트:
affinity-pod가 anchor-pod와 **정확히 동일한 NODE**에 배포된 것을 확인할 수 있습니다. 이것이 파드 어피니티 규칙이 성공적으로 동작했다는 증거입니다.

4단계: 실습 리소스 정리
실습이 끝나면 생성했던 모든 Pod를 삭제합니다.

Bash

$ kubectl delete pod anchor-pod affinity-pod

